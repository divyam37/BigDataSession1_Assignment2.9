Hive is a data warehouse infrastructure tool to process structured data in Hadoop. It resides on top of Hadoop to summarize Big Data, and makes querying and analyzing easy.
The main components of Hive are:
UI – The user interface for users to submit queries and other operations to the system. As of 2011 the system had a command line interface and a web based GUI was being developed.
Driver – The component which receives the queries. This component implements the notion of session handles and provides execute and fetch APIs modeled on JDBC/ODBC interfaces.
Compiler – The component that parses the query, does semantic analysis on the different query blocks and query expressions and eventually generates an execution plan with the help of the table and partition metadata looked up from the metastore.
Metastore – The component that stores all the structure information of the various tables and partitions in the warehouse including column and column type information, the serializers and deserializers necessary to read and write data and the corresponding HDFS files where the data is stored.
Execution Engine – The component which executes the execution plan created by the compiler. The plan is a DAG of stages. The execution engine manages the dependencies between these different stages of the plan and executes these stages on the appropriate system components.
Working of hive:
The Hive interface such as Command Line or Web UI sends query to Driver (any database driver such as JDBC, ODBC, etc.) to execute.The driver takes the help of query compiler that parses the query to check the syntax and query plan or the requirement of query.The compiler sends metadata request to Metastore (any database).Metastore sends metadata as a response to the compiler.The compiler checks the requirement and resends the plan to the driver. Up to here, the parsing and compiling of a query is complete.The driver sends the execute plan to the execution engine.Internally, the process of execution job is a MapReduce job. The execution engine sends the job to JobTracker, which is in Name node and it assigns this job to TaskTracker, which is in Data node. Here, the query executes MapReduce job.Meanwhile in execution, the execution engine can execute metadata operations with Metastore.The execution engine receives the results from Data nodes.The execution engine sends those resultant values to the driver.The driver sends the results to Hive Interfaces.
